{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "x-phgIC0RczR",
        "Lf9Oy3_hRf_3",
        "77_UVTk1Rh7t",
        "OqnXsNv3RnbF",
        "BJjCfk-NRtRe",
        "KGAPC-Y5RyDX"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Requirements"
      ],
      "metadata": {
        "id": "x-phgIC0RczR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "id": "L9zkkyLKReU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ddbd316-034a-4e57-f71e-93b139797c3d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.7.2-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports"
      ],
      "metadata": {
        "id": "Lf9Oy3_hRf_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchinfo import summary\n",
        "from my_transformer import Transformer\n",
        "from my_train import train\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "b6pQqekcQi9E"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Loading and Preprocessing"
      ],
      "metadata": {
        "id": "77_UVTk1Rh7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "English_sens = ['i am fine', 'you are fine', 'he is fine', 'they are fine']\n",
        "Spanish_sens = ['<st> yo estoy bien <end>', '<st> tu eres bien <end>',\n",
        "                '<st> el es bien <end>', '<st> ellos son bien <end>']"
      ],
      "metadata": {
        "id": "bXac_iMSQwT0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_tokens(tokenizer, dataset):\n",
        "  for sample in dataset:\n",
        "    yield tokenizer(sample)\n",
        "\n",
        "tokenizer_src = get_tokenizer('basic_english', language='en')\n",
        "tokenizer_trg = get_tokenizer('toktok', language='es')\n",
        "\n",
        "vocab_src = build_vocab_from_iterator(create_tokens(tokenizer_src, English_sens), specials=[\"<oov>\", \"<sos>\"])\n",
        "vocab_src.set_default_index(vocab_src[\"<oov>\"])\n",
        "print(f\"Our vocabulary is made of {len(vocab_src)} tokens-index pairs.\")\n",
        "\n",
        "vocab_trg = build_vocab_from_iterator(create_tokens(tokenizer_trg, Spanish_sens), specials=[\"<oov>\", \"<sos>\"])\n",
        "vocab_trg.set_default_index(vocab_trg[\"<oov>\"])\n",
        "print(f\"Our vocabulary is made of {len(vocab_trg)} tokens-index pairs.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYnb4Uv3Qy2Z",
        "outputId": "1d99005d-d288-4f9c-e848-00ba7f371bcc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our vocabulary is made of 10 tokens-index pairs.\n",
            "Our vocabulary is made of 13 tokens-index pairs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx_to_word_src = {vocab_src[w]:w for w in vocab_src.get_itos()}\n",
        "idx_to_word_trg = {vocab_trg[w]:w for w in vocab_trg.get_itos()}"
      ],
      "metadata": {
        "id": "O1938M9lQ-QW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_pipeline_src = lambda x: vocab_src(tokenizer_src(x))\n",
        "text_pipeline_trg = lambda x: vocab_trg(tokenizer_trg(x))"
      ],
      "metadata": {
        "id": "eGYI_BaKRAAB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sent_padding(sent_vec, maxlen):\n",
        "  sent_vec = torch.tensor(sent_vec)\n",
        "  maxlen -= len(sent_vec)\n",
        "  return F.pad(sent_vec, (0, maxlen))"
      ],
      "metadata": {
        "id": "mjiWE08_RHPY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "\n",
        "  def __init__(self, SRC, TRG, seq_len_src, seq_len_trg, device):\n",
        "    self.SRC = SRC\n",
        "    self.TRG = TRG\n",
        "    self.seq_len_src = seq_len_src\n",
        "    self.seq_len_trg = seq_len_trg\n",
        "    self.device = device\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.SRC)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    src, trg = self.SRC[idx], self.TRG[idx]\n",
        "\n",
        "    src = sent_padding(text_pipeline_src(src), maxlen=self.seq_len_src)\n",
        "    trg = sent_padding(text_pipeline_trg(trg), maxlen=self.seq_len_trg)\n",
        "\n",
        "    return src.to(self.device), trg.to(self.device)"
      ],
      "metadata": {
        "id": "ls1MblFcRLC_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len_src = 8\n",
        "seq_len_trg_PRIME = 6\n",
        "seq_len_trg = seq_len_trg_PRIME - 1\n",
        "\n",
        "batch_size = 2\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "dataloader = DataLoader(MyDataset(English_sens, Spanish_sens, seq_len_src, seq_len_trg_PRIME, device), batch_size=batch_size)"
      ],
      "metadata": {
        "id": "Hlqo-3t0N0bj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Definition"
      ],
      "metadata": {
        "id": "OqnXsNv3RnbF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab = len(vocab_src)\n",
        "trg_vocab = len(vocab_trg)\n",
        "d_model = 32\n",
        "N = 1\n",
        "heads = 2\n",
        "max_seq_len = max(seq_len_src, seq_len_trg)\n",
        "\n",
        "\n",
        "model = Transformer(src_vocab, trg_vocab, d_model, N, heads, max_seq_len).to(device)\n",
        "summary(model, [(batch_size, seq_len_src), (batch_size, seq_len_trg)], dtypes=[torch.long, torch.long])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoJTk6QjNbVO",
        "outputId": "75343161-02b9-4ddd-ef9f-247958d3e9e3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "===============================================================================================\n",
              "Layer (type:depth-idx)                        Output Shape              Param #\n",
              "===============================================================================================\n",
              "Transformer                                   [2, 5, 13]                --\n",
              "├─Encoder: 1-1                                [2, 8, 32]                --\n",
              "│    └─IO_Embedding: 2-1                      [2, 8, 32]                --\n",
              "│    │    └─Embedding: 3-1                    [2, 8, 32]                320\n",
              "│    └─PositionalEncoding: 2-2                [2, 8, 32]                --\n",
              "│    └─ModuleList: 2-3                        --                        --\n",
              "│    │    └─SingleEncoderLayer: 3-2           [2, 8, 32]                137,504\n",
              "├─Decoder: 1-2                                [2, 5, 32]                --\n",
              "│    └─IO_Embedding: 2-4                      [2, 5, 32]                --\n",
              "│    │    └─Embedding: 3-3                    [2, 5, 32]                416\n",
              "│    └─PositionalEncoding: 2-5                [2, 5, 32]                --\n",
              "│    └─ModuleList: 2-6                        --                        --\n",
              "│    │    └─SingleDecoderLayer: 3-4           [2, 5, 32]                141,792\n",
              "├─Linear: 1-3                                 [2, 5, 13]                429\n",
              "===============================================================================================\n",
              "Total params: 280,461\n",
              "Trainable params: 280,461\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 0.56\n",
              "===============================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.50\n",
              "Params size (MB): 1.12\n",
              "Estimated Total Size (MB): 1.62\n",
              "==============================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "BJjCfk-NRtRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1000\n",
        "print_step = 100\n",
        "lr = 1e-3\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "loss = train(model, optimizer, dataloader, epochs, print_step)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niDOUbEEOC_r",
        "outputId": "b6859f7d-d19f-4e85-ce20-a278f0d32285"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 -> Loss:  2.87587452\n",
            "Epoch: 101 -> Loss:  0.04464610\n",
            "Epoch: 201 -> Loss:  0.01437878\n",
            "Epoch: 301 -> Loss:  0.00755641\n",
            "Epoch: 401 -> Loss:  0.00375758\n",
            "Epoch: 501 -> Loss:  0.00337414\n",
            "Epoch: 601 -> Loss:  0.00205486\n",
            "Epoch: 701 -> Loss:  0.00172235\n",
            "Epoch: 801 -> Loss:  0.00129193\n",
            "Epoch: 901 -> Loss:  0.00074596\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Translation"
      ],
      "metadata": {
        "id": "KGAPC-Y5RyDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def translate(sentence, device):\n",
        "\n",
        "  sen_SRC = sent_padding(text_pipeline_src(sentence), maxlen=seq_len_src).unsqueeze(0).to(device)\n",
        "\n",
        "  sen_TRG = '<st>'\n",
        "\n",
        "  while '<end>' not in sen_TRG:\n",
        "\n",
        "    length = len(sen_TRG.split())\n",
        "\n",
        "    trg_input = sent_padding(text_pipeline_trg(sen_TRG), maxlen=seq_len_trg).unsqueeze(0)[:, :-1].to(device)\n",
        "\n",
        "    preds = model(sen_SRC, trg_input, src_mask=None, trg_mask=None).squeeze()\n",
        "\n",
        "    next_word_idx = torch.argmax(preds, dim=-1)[length-1] #IMPORTANT\n",
        "    \n",
        "    sen_TRG += (' ' + idx_to_word_trg[next_word_idx.item()])\n",
        "\n",
        "  return sen_TRG\n"
      ],
      "metadata": {
        "id": "ftWxDHd9Ppif"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'they are fine'\n",
        "\n",
        "print(translate(sentence, device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-wbVnJSQEvp",
        "outputId": "02246c8a-3599-4444-a969-8b635dcd7fd6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<st> ellos son bien <end>\n"
          ]
        }
      ]
    }
  ]
}