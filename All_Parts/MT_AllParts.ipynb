{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "LlvhOI7JdfYV",
        "43kMYrPVC4LV",
        "ykNT-IOCCUZM",
        "NWuj5EuYCZbN",
        "1mX2YFDJCcgz",
        "-pam5fNJCeaf",
        "CwcmpHn3Cllw",
        "PYwI8JYuCpYf",
        "ts6zOg4LCsD8",
        "Yqsd7wDNEUle",
        "7FbH5HUBm3sl",
        "RnC2v0BCBrwF",
        "1R3sOGarOBql"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Requirements"
      ],
      "metadata": {
        "id": "LlvhOI7JdfYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWNhx3Uzdg27",
        "outputId": "2380b454-235f-4da3-bab4-261b964f6f19"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.7.2-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports"
      ],
      "metadata": {
        "id": "43kMYrPVC4LV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchinfo import summary\n",
        "import numpy as np\n",
        "import math\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "fIhh2go7C4RL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#IO Embedding"
      ],
      "metadata": {
        "id": "ykNT-IOCCUZM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![picture](https://miro.medium.com/v2/resize:fit:720/format:webp/1*2vyKzFlzIHfSmOU_lnQE4A.png)"
      ],
      "metadata": {
        "id": "8PQN93QaQfAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IO_Embedding(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch_size, seq_len_SRC/TRG]\n",
        "        return self.embed(x) # [batch_size, seq_len_SRC/TRG, d_model]"
      ],
      "metadata": {
        "id": "0INKFg5MPx9p"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![picture](https://miro.medium.com/v2/resize:fit:524/format:webp/1*yWGV9ck-0ltfV2wscUeo7Q.png)\n",
        "\n",
        "![picture](https://miro.medium.com/v2/resize:fit:564/format:webp/1*SgNlyFaHH8ljBbpCupDhSQ.png)"
      ],
      "metadata": {
        "id": "GGat6FQ4QjHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_len):\n",
        "        '''\n",
        "        Why considering max_seq_len?\n",
        "          Since seq_len_SRC is not necessarily equal to seq_len_TRG and since\n",
        "          we want to use this class both for SRC and TRG sentences, we set:\n",
        "                max_seq_len = MAX(seq_len_SRC, seq_len_TRG - 1).\n",
        "        '''\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        positional_emb = torch.zeros(max_seq_len, d_model)\n",
        "\n",
        "        for pos in range(max_seq_len):\n",
        "            for i in range(0, d_model, 2):\n",
        "                positional_emb[pos, i] = math.sin(pos / (10000 ** (i/d_model)))\n",
        "                positional_emb[pos, i + 1] = math.cos(pos / (10000 ** (i/d_model)))\n",
        "                \n",
        "        self.register_buffer('positional_emb', positional_emb)\n",
        "        self.positional_emb.requires_grad = False\n",
        "    \n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        x is the embedded vector, coming from the previous class as the output.\n",
        "        The reason we increase the embedding values before addition is to make the\n",
        "        positional encoding relatively smaller. This means the original meaning in\n",
        "        the embedding vector wont be lost when we add them together.\n",
        "        '''\n",
        "        # x: [batch_size, seq_len_SRC/TRG, d_model]      \n",
        "        x = x * math.sqrt(self.d_model)\n",
        "\n",
        "        _, seq_len, _ = x.size()\n",
        "        x = x + self.positional_emb[:seq_len, :]\n",
        "        # self.positional_emb[:seq_len, :]: [seq_len_SRC/TRG, d_model]\n",
        "        # x:                                [batch_size, seq_len_SRC/TRG, d_model] \n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "pP7UvqYnQYep"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Attention"
      ],
      "metadata": {
        "id": "NWuj5EuYCZbN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![picture](https://miro.medium.com/v2/resize:fit:750/format:webp/1*1tsRtfaY9z6HxmERYhw8XQ.png)"
      ],
      "metadata": {
        "id": "60TYlg3cWQid"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "V, K and Q stand for ‘key’, ‘value’ and ‘query’. These are terms used in attention functions, but honestly, I don’t think explaining this terminology is particularly important for understanding the model.\n",
        "\n",
        "In the case of the Encoder, V, K and G will simply be identical copies of the embedding vector (plus positional encoding). They will have the dimensions Batch_size * seq_len * d_model.\n",
        "\n",
        "In multi-head attention we split the embedding vector into N heads, so they will then have the dimensions batch_size * N * seq_len * (d_model / N).\n",
        "\n",
        "This final dimension (d_model / N ) we will refer to as d_k.\n",
        "\n"
      ],
      "metadata": {
        "id": "A_pxHvlRXJTE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![pictures](https://miro.medium.com/v2/resize:fit:224/format:webp/1*15E9qKg9bKnWdSRWCyY2iA.png)\n",
        "![pictures](https://miro.medium.com/v2/resize:fit:640/format:webp/1*evdACdTOBT5j1g1nXialBg.png)\n"
      ],
      "metadata": {
        "id": "i-kW5ZY-o0ep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def attention(q, k, v, d_k, mask=None, dropout=None):\n",
        "    #(1) q, k, v : [batch_size, N, seq_len_SRC/TRG, d_k]\n",
        "    #(2) k, v : [batch_size, N, seq_len_SRC, d_k] ---- q: [batch_size, N, seq_len_TRG, d_k]\n",
        "    #(1) ---> First Attention Layers in Encoder and Decoder\n",
        "    #(2) ---> Middle Attention Layer in Decoder\n",
        "\n",
        "    scores = torch.matmul(q, k.permute(0, 1, 3, 2)) /  math.sqrt(d_k)\n",
        "    #(1) scores: [batch_size, N, seq_len_SRC/TRG, seq_len_SRC/TRG]\n",
        "    #(2) scores: [batch_size, N, seq_len_TRG, seq_len_SRC]\n",
        "    if mask is not None:\n",
        "        mask = mask.unsqueeze(1)\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "    scores = F.softmax(scores, dim=-1)\n",
        "    \n",
        "    if dropout is not None:\n",
        "        scores = dropout(scores)\n",
        "        \n",
        "    output = torch.matmul(scores, v)\n",
        "    #(1) output: [batch_size, N, seq_len_SRC/TRG, d_k]\n",
        "    #(2) output: [batch_size, N, seq_len_TRG, d_k]\n",
        "    return output"
      ],
      "metadata": {
        "id": "uRaOUdBIo6Uk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, heads, d_model, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.N = heads\n",
        "        self.d_model = d_model\n",
        "        self.d_k = d_model // heads\n",
        "        \n",
        "        self.q_linear = nn.Linear(d_model, d_model)\n",
        "        self.v_linear = nn.Linear(d_model, d_model)\n",
        "        self.k_linear = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.out = nn.Linear(d_model, d_model)\n",
        "    \n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        #(1) q, k, v : [batch_size, seq_len_SRC/TRG, d_model]\n",
        "        #(2) k, v : [batch_size, seq_len_SRC, d_model] ---- q: [batch_size, seq_len_TRG, d_model]\n",
        "        #(1) ---> First Attention Layers in Encoder and Decoder\n",
        "        #(2) ---> Middle Attention Layer in Decoder\n",
        "\n",
        "        batch_size = q.size(0)\n",
        "                \n",
        "        k = self.k_linear(k).view(batch_size, -1, self.N, self.d_k).permute(0, 2, 1, 3)\n",
        "        q = self.q_linear(q).view(batch_size, -1, self.N, self.d_k).permute(0, 2, 1, 3)\n",
        "        v = self.v_linear(v).view(batch_size, -1, self.N, self.d_k).permute(0, 2, 1, 3)\n",
        "        #(1) q, k, v : [batch_size, N, seq_len_SRC/TRG, d_k]\n",
        "        #(2) k, v : [batch_size, N, seq_len_SRC, d_k] ---- q: [batch_size, N, seq_len_TRG, d_k]\n",
        "\n",
        "        scores = attention(q, k, v, self.d_k, mask, self.dropout)\n",
        "        #(1) scores: [batch_size, N, seq_len_SRC/TRG, d_k]\n",
        "        #(2) scores: [batch_size, N, seq_len_TRG, d_k]\n",
        "        \n",
        "        concat = scores.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_model)\n",
        "        #(1) concat: [batch_size, seq_len_SRC/TRG, d_model]\n",
        "        #(2) concat: [batch_size, seq_len_TRG, d_model]\n",
        "        output = self.out(concat)\n",
        "        #(1) output: [batch_size, seq_len_SRC/TRG, d_model]\n",
        "        #(2) output: [batch_size, seq_len_TRG, d_model]\n",
        "    \n",
        "        return output"
      ],
      "metadata": {
        "id": "wy5ChZ2XVySa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Norm"
      ],
      "metadata": {
        "id": "1mX2YFDJCcgz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![picture](https://github.com/hyunwoongko/transformer/raw/master/image/layer_norm.jpg)"
      ],
      "metadata": {
        "id": "adK7fBqZuXAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Norm(nn.Module):\n",
        "    def __init__(self, d_model, eps = 1e-6):\n",
        "        super().__init__()\n",
        "    \n",
        "        self.d_model = d_model\n",
        "        self.eps = eps\n",
        "\n",
        "        self.Gamma = nn.Parameter(torch.ones(self.d_model)) #learnable\n",
        "        self.Beta = nn.Parameter(torch.zeros(self.d_model)) #learnable\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch_size, seq_len, d_model]\n",
        "        mio = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, unbiased=False, keepdim=True)\n",
        "\n",
        "        x_hat = (x - mio) / (torch.sqrt(var + self.eps))\n",
        "        y = self.Gamma * x_hat + self.Beta\n",
        "        # y: [batch_size, seq_len, d_model]\n",
        "        return y"
      ],
      "metadata": {
        "id": "3fGSStG9uGPx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FF"
      ],
      "metadata": {
        "id": "-pam5fNJCeaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff=2048, dropout = 0.1):\n",
        "        super().__init__() \n",
        "\n",
        "        self.lin1 = nn.Linear(d_model, d_ff)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.lin2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch_size, seq_len, d_model]\n",
        "        x = self.dropout(F.relu(self.lin1(x)))\n",
        "        # x: [batch_size, seq_len, d_ff]\n",
        "        x = self.lin2(x)\n",
        "        # x: [batch_size, seq_len, d_model]\n",
        "        return x"
      ],
      "metadata": {
        "id": "U3Z-AsB0tV-K"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Encoder and Decoder Layers"
      ],
      "metadata": {
        "id": "CwcmpHn3Cllw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![picture](https://miro.medium.com/v2/resize:fit:720/format:webp/1*2vyKzFlzIHfSmOU_lnQE4A.png)"
      ],
      "metadata": {
        "id": "ogCsOOrlvq1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SingleEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads, dropout = 0.1):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.norm1 = Norm(d_model)\n",
        "        self.norm2 = Norm(d_model)\n",
        "        self.attention = MultiHeadAttention(heads, d_model)\n",
        "        self.ff = FeedForward(d_model)\n",
        "        self.drp1 = nn.Dropout(dropout)\n",
        "        self.drp2 = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        # x: [batch_size, seq_len_SRC, d_model]\n",
        "        x_copied = x\n",
        "        x = self.attention(x, x, x, mask) # Attention\n",
        "        # x: [batch_size, seq_len_SRC, d_model]\n",
        "        x = self.norm1(x_copied + self.drp1(x)) # Add & Norm\n",
        "        # x: [batch_size, seq_len_SRC, d_model]\n",
        "        \n",
        "        x_copied = x\n",
        "        x = self.ff(x) # Feed forward\n",
        "        # x: [batch_size, seq_len_SRC, d_model]\n",
        "        x = self.norm2(x_copied + self.drp2(x)) # Add & Norm\n",
        "        # x: [batch_size, seq_len_SRC, d_model]\n",
        "        return x"
      ],
      "metadata": {
        "id": "_X98jX0rslUJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SingleDecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads, dropout=0.1):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.norm1 = Norm(d_model)\n",
        "        self.norm2 = Norm(d_model)\n",
        "        self.norm3 = Norm(d_model)\n",
        "        \n",
        "        self.drp1 = nn.Dropout(dropout)\n",
        "        self.drp2 = nn.Dropout(dropout)\n",
        "        self.drp3 = nn.Dropout(dropout)\n",
        "        \n",
        "        self.attention1 = MultiHeadAttention(heads, d_model)\n",
        "        self.attention2 = MultiHeadAttention(heads, d_model)\n",
        "\n",
        "        self.ff = FeedForward(d_model)\n",
        "\n",
        "    def forward(self, y, enc, src_mask, trg_mask):\n",
        "        # y: [batch_size, seq_len_TRG, d_model]\n",
        "        y_copied = y\n",
        "        y = self.attention1(y, y, y, trg_mask) # Attention: Bottom\n",
        "        y = self.norm1(y_copied + self.drp1(y)) # Add & Norm\n",
        "        # y: [batch_size, seq_len_TRG, d_model]\n",
        "\n",
        "        # enc: [batch_size, seq_len_SRC, d_model]\n",
        "        enc = self.attention2(y, enc, enc, src_mask) # Attention: Middle\n",
        "        # enc: [batch_size, seq_len_TRG, d_model] ---> (2)\n",
        "        enc = self.norm2(y + self.drp2(enc)) # Add & Norm : Very important\n",
        "        # enc: [batch_size, seq_len_TRG, d_model] ---> (2)\n",
        "\n",
        "        enc_copied = enc\n",
        "        enc = self.ff(enc) # Feed forward: Up\n",
        "        # enc: [batch_size, seq_len_TRG, d_model]\n",
        "        out = self.norm3(enc_copied + self.drp3(enc)) # Add & Norm\n",
        "        # out: [batch_size, seq_len_TRG, d_model]\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "lTAKHtzWxKOK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Encoder and Decoder"
      ],
      "metadata": {
        "id": "PYwI8JYuCpYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, N, heads, max_seq_len):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.N = N # how many encoding layer\n",
        "        self.emb = IO_Embedding(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoding(d_model, max_seq_len)\n",
        "        self.layers = nn.ModuleList([SingleEncoderLayer(d_model, heads) for _ in range(N)])\n",
        "\n",
        "    def forward(self, src, mask):\n",
        "        # x: [batch_size, seq_len_SRC]\n",
        "        x = self.emb(src)\n",
        "        # x: [batch_size, seq_len_SRC, d_model]\n",
        "        x = self.pe(x)\n",
        "        # x: [batch_size, seq_len_SRC, d_model]\n",
        "\n",
        "        for i in range(self.N):\n",
        "            x = self.layers[i](x, mask)\n",
        "        # x: [batch_size, seq_len_SRC, d_model]\n",
        "        return x"
      ],
      "metadata": {
        "id": "TyMC3e_b5TK4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, N, heads, max_seq_len):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.N = N\n",
        "        self.emb = IO_Embedding(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoding(d_model, max_seq_len)\n",
        "        self.layers = nn.ModuleList([SingleDecoderLayer(d_model, heads) for _ in range(N)])\n",
        "\n",
        "    def forward(self, trg, enc, src_mask, trg_mask):\n",
        "        # x: [batch_size, seq_len_TRG]\n",
        "        x = self.emb(trg)\n",
        "        # x: [batch_size, seq_len_TRG, d_model]\n",
        "        x = self.pe(x)\n",
        "        # x: [batch_size, seq_len_TRG, d_model]\n",
        "\n",
        "        for i in range(self.N):\n",
        "            x = self.layers[i](x, enc, src_mask, trg_mask)\n",
        "        # x: [batch_size, seq_len_TRG, d_model]\n",
        "        return x"
      ],
      "metadata": {
        "id": "JOiZK8rf8DpF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transformer"
      ],
      "metadata": {
        "id": "ts6zOg4LCsD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab, trg_vocab, d_model, N, heads, max_seq_len):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = Encoder(src_vocab, d_model, N, heads, max_seq_len)\n",
        "        self.decoder = Decoder(trg_vocab, d_model, N, heads, max_seq_len)\n",
        "\n",
        "        self.out = nn.Linear(d_model, trg_vocab)\n",
        "\n",
        "    def forward(self, src, trg, src_mask=None, trg_mask=None):\n",
        " \n",
        "        # src: [batch_size, seq_len_SRC]\n",
        "        # trg: [batch_size, seq_len_TRG]\n",
        "        enc = self.encoder(src, src_mask)\n",
        "        # enc: [batch_size, seq_len_SRC, d_model]\n",
        "        dec = self.decoder(trg, enc, src_mask, trg_mask)\n",
        "        # dec: [batch_size, seq_len_TRG, d_model]\n",
        "        output = self.out(dec)\n",
        "        # output: [batch_size, seq_len_TRG, trg_vocab]\n",
        "        return output"
      ],
      "metadata": {
        "id": "81JCqOOT8vVv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train"
      ],
      "metadata": {
        "id": "Yqsd7wDNEUle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optim, dataloader, epochs, print_step=1):\n",
        "    \n",
        "    model.train()  \n",
        "    total_loss = []\n",
        "\n",
        "    for epoch in range(epochs): \n",
        "\n",
        "        losses, step = 0, 0\n",
        "\n",
        "        for idx, (src, trg) in enumerate(dataloader):\n",
        "            # src: [batch_size, seq_len_SRC]\n",
        "            # trg: [batch_size, seq_len_TRG_PRIME]\n",
        "\n",
        "            trg_input = trg[:, :-1]  \n",
        "            # trg_input: [batch_size, seq_len_TRG=seq_len_TRG_PRIME-1]\n",
        "            \n",
        "            preds = model(src, trg_input, src_mask=None, trg_mask=None)\n",
        "            # preds: [batch_size, seq_len_TRG, trg_vocab]\n",
        "\n",
        "            ys = trg[:, 1:].contiguous() #Right shifted\n",
        "            # ys: [batch_size, seq_len_TRG]\n",
        "\n",
        "            optim.zero_grad()\n",
        "\n",
        "            loss = F.cross_entropy(preds.view(-1, preds.size(-1)), ys.view(-1))\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            \n",
        "            losses += loss\n",
        "            step += 1\n",
        "        \n",
        "        total_loss.append(losses.item() / step)\n",
        "\n",
        "        if epoch % print_step == 0:\n",
        "\n",
        "          print(f\"Epoch: {epoch+1} -> Loss: {total_loss[-1]: .8f}\")\n",
        "\n",
        "    return total_loss"
      ],
      "metadata": {
        "id": "ruFfG6sv9egk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data"
      ],
      "metadata": {
        "id": "7FbH5HUBm3sl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "English_sens = ['i am fine', 'you are fine', 'he is fine', 'they are fine']\n",
        "Spanish_sens = ['<st> yo estoy bien <end>', '<st> tu eres bien <end>',\n",
        "                '<st> el es bien <end>', '<st> ellos son bien <end>']\n",
        "\n",
        "tokenizer_src = get_tokenizer('basic_english', language='en')\n",
        "tokenizer_trg = get_tokenizer('toktok', language='es')\n",
        "\n",
        "seq_len_src = 10\n",
        "seq_len_trg_PRIME = 17\n",
        "seq_len_trg = seq_len_trg_PRIME - 1\n",
        "\n",
        "def create_tokens(tokenizer, dataset):\n",
        "  for sample in dataset:\n",
        "    yield tokenizer(sample)\n",
        "\n",
        "vocab_src = build_vocab_from_iterator(create_tokens(tokenizer_src, English_sens), specials=[\"<oov>\", \"<sos>\"])\n",
        "vocab_src.set_default_index(vocab_src[\"<oov>\"])\n",
        "print(f\"Our vocabulary is made of {len(vocab_src)} tokens-index pairs.\")\n",
        "\n",
        "vocab_trg = build_vocab_from_iterator(create_tokens(tokenizer_trg, Spanish_sens), specials=[\"<oov>\", \"<sos>\"])\n",
        "vocab_trg.set_default_index(vocab_trg[\"<oov>\"])\n",
        "print(f\"Our vocabulary is made of {len(vocab_trg)} tokens-index pairs.\")\n",
        "\n",
        "idx_to_word_src = {vocab_src[w]:w for w in vocab_src.get_itos()}\n",
        "idx_to_word_trg = {vocab_trg[w]:w for w in vocab_trg.get_itos()}\n",
        "\n",
        "text_pipeline_src = lambda x: vocab_src(tokenizer_src(x))\n",
        "text_pipeline_trg = lambda x: vocab_trg(tokenizer_trg(x))\n",
        "\n",
        "def sent_padding(sent_vec, maxlen):\n",
        "  sent_vec = torch.tensor(sent_vec)\n",
        "  maxlen -= len(sent_vec)\n",
        "  return F.pad(sent_vec, (0, maxlen))\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "\n",
        "  def __init__(self, SRC, TRG, seq_len_src, seq_len_trg, device):\n",
        "    self.SRC = SRC\n",
        "    self.TRG = TRG\n",
        "    self.seq_len_src = seq_len_src\n",
        "    self.seq_len_trg = seq_len_trg\n",
        "    self.device = device\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.SRC)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    src, trg = self.SRC[idx], self.TRG[idx]\n",
        "\n",
        "    src = sent_padding(text_pipeline_src(src), maxlen=self.seq_len_src)\n",
        "    trg = sent_padding(text_pipeline_trg(trg), maxlen=self.seq_len_trg)\n",
        "\n",
        "    return src.to(self.device), trg.to(self.device)\n",
        "\n",
        "batch_size = 2\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "dataloader = DataLoader(MyDataset(English_sens, Spanish_sens, seq_len_src, seq_len_trg_PRIME, device), batch_size=batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLVmy9RVm5IS",
        "outputId": "47e08c04-b5f5-4d8c-aa8c-8fdda4882204"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our vocabulary is made of 10 tokens-index pairs.\n",
            "Our vocabulary is made of 13 tokens-index pairs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "RnC2v0BCBrwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab = len(vocab_src)\n",
        "trg_vocab = len(vocab_trg)\n",
        "d_model = 32\n",
        "N = 1\n",
        "heads = 2\n",
        "max_seq_len = max(seq_len_src, seq_len_trg)\n",
        "\n",
        "\n",
        "model = Transformer(src_vocab, trg_vocab, d_model, N, heads, max_seq_len).to(device)\n",
        "\n",
        "summary(model, [(batch_size, seq_len_src), (batch_size, seq_len_trg)], dtypes=[torch.long, torch.long])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQ-mI3xPNm3p",
        "outputId": "201a24a4-85fa-4d31-cf3c-b80a99d13d52"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "===============================================================================================\n",
              "Layer (type:depth-idx)                        Output Shape              Param #\n",
              "===============================================================================================\n",
              "Transformer                                   [2, 16, 13]               --\n",
              "├─Encoder: 1-1                                [2, 10, 32]               --\n",
              "│    └─IO_Embedding: 2-1                      [2, 10, 32]               --\n",
              "│    │    └─Embedding: 3-1                    [2, 10, 32]               320\n",
              "│    └─PositionalEncoding: 2-2                [2, 10, 32]               --\n",
              "│    └─ModuleList: 2-3                        --                        --\n",
              "│    │    └─SingleEncoderLayer: 3-2           [2, 10, 32]               137,504\n",
              "├─Decoder: 1-2                                [2, 16, 32]               --\n",
              "│    └─IO_Embedding: 2-4                      [2, 16, 32]               --\n",
              "│    │    └─Embedding: 3-3                    [2, 16, 32]               416\n",
              "│    └─PositionalEncoding: 2-5                [2, 16, 32]               --\n",
              "│    └─ModuleList: 2-6                        --                        --\n",
              "│    │    └─SingleDecoderLayer: 3-4           [2, 16, 32]               141,792\n",
              "├─Linear: 1-3                                 [2, 16, 13]               429\n",
              "===============================================================================================\n",
              "Total params: 280,461\n",
              "Trainable params: 280,461\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 0.56\n",
              "===============================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 1.00\n",
              "Params size (MB): 1.12\n",
              "Estimated Total Size (MB): 2.12\n",
              "==============================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1000\n",
        "print_step = 50\n",
        "lr = 1e-3\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "loss = train(model, optimizer, dataloader, epochs, print_step)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sb1SKljbNlQV",
        "outputId": "e6fff8c0-bfac-427e-faff-b3a8b7f8cd6e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 -> Loss:  2.33081889\n",
            "Epoch: 51 -> Loss:  0.07398581\n",
            "Epoch: 101 -> Loss:  0.02989515\n",
            "Epoch: 151 -> Loss:  0.01468811\n",
            "Epoch: 201 -> Loss:  0.00888169\n",
            "Epoch: 251 -> Loss:  0.00636232\n",
            "Epoch: 301 -> Loss:  0.00446561\n",
            "Epoch: 351 -> Loss:  0.00289530\n",
            "Epoch: 401 -> Loss:  0.00263622\n",
            "Epoch: 451 -> Loss:  0.00211905\n",
            "Epoch: 501 -> Loss:  0.00196347\n",
            "Epoch: 551 -> Loss:  0.00142200\n",
            "Epoch: 601 -> Loss:  0.00122349\n",
            "Epoch: 651 -> Loss:  0.00140041\n",
            "Epoch: 701 -> Loss:  0.00105487\n",
            "Epoch: 751 -> Loss:  0.00085137\n",
            "Epoch: 801 -> Loss:  0.00072205\n",
            "Epoch: 851 -> Loss:  0.00062553\n",
            "Epoch: 901 -> Loss:  0.00060675\n",
            "Epoch: 951 -> Loss:  0.00060869\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Translation"
      ],
      "metadata": {
        "id": "1R3sOGarOBql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def translate(sentence, device):\n",
        "\n",
        "  sen_SRC = sent_padding(text_pipeline_src(sentence), maxlen=seq_len_src).unsqueeze(0).to(device)\n",
        "\n",
        "  sen_TRG = '<st>'\n",
        "\n",
        "  while '<end>' not in sen_TRG:\n",
        "\n",
        "    length = len(sen_TRG.split())\n",
        "\n",
        "    trg_input = sent_padding(text_pipeline_trg(sen_TRG), maxlen=seq_len_trg).unsqueeze(0)[:, :-1].to(device)\n",
        "\n",
        "    preds = model(sen_SRC, trg_input, src_mask=None, trg_mask=None).squeeze()\n",
        "\n",
        "    next_word_idx = torch.argmax(preds, dim=-1)[length-1] #IMPORTANT\n",
        "    \n",
        "    sen_TRG += (' ' + idx_to_word_trg[next_word_idx.item()])\n",
        "\n",
        "  return sen_TRG\n"
      ],
      "metadata": {
        "id": "7KnO31B9Zqz0"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'they are fine'\n",
        "\n",
        "print(translate(sentence, device))"
      ],
      "metadata": {
        "id": "6dY8k5xYO_oY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ccad61f-9f96-450f-f46f-ad87f82cdaaf"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<st> ellos son bien <end>\n"
          ]
        }
      ]
    }
  ]
}